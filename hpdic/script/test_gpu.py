import torch
import time

# 1. æ£€æŸ¥è®¾å¤‡
if not torch.cuda.is_available():
    print("âŒ æ²¡æ£€æµ‹åˆ° GPUï¼")
    exit()

device = torch.device("cuda")
print(f"âœ… æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(0)}")
print(f"   æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

# 2. å‡†å¤‡æ•°æ® (æä¸¤ä¸ªå·¨å¤§çš„çŸ©é˜µï¼Œåƒæ»¡ Tensor Core)
# 10000x10000 çš„çŸ©é˜µï¼ŒFP16 æ¨¡å¼ (A100 æ“…é•¿è¿™ä¸ª)
N = 10000
dtype = torch.float16 

print(f"\nğŸš€ å¼€å§‹æµ‹è¯•çŸ©é˜µä¹˜æ³• ({N}x{N}, FP16)...")
a = torch.randn(N, N, device=device, dtype=dtype)
b = torch.randn(N, N, device=device, dtype=dtype)

# 3. é¢„çƒ­ (Warm up)
for _ in range(5):
    _ = torch.matmul(a, b)
torch.cuda.synchronize()

# 4. æ­£å¼æµ‹é€Ÿ
start_time = time.time()
num_iters = 100
for _ in range(num_iters):
    c = torch.matmul(a, b)
torch.cuda.synchronize()
end_time = time.time()

avg_time = (end_time - start_time) / num_iters
tflops = (2 * N**3) / (avg_time * 1e12)

print(f"âœ… å®Œæˆï¼å¹³å‡è€—æ—¶: {avg_time*1000:.2f} ms")
print(f"âš¡ ä¼°ç®—æ€§èƒ½: {tflops:.2f} TFLOPS")
